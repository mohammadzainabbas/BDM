{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40395952-efee-4ce9-a21b-fe326add3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # disable warnings\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import csv, sys\n",
    "import dateutil.parser\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import (StringType, DoubleType, TimestampType, NullType, IntegerType, StructType, StructField)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed05d9",
   "metadata": {},
   "source": [
    "Config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a97ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For IPython\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # To show all output after each cell execution (instead of the last output)\n",
    "\n",
    "# For HDFS\n",
    "HDFS_DEFAULT = \"hdfs://alakazam.fib.upc.es:27000\"\n",
    "HDFS_USER = \"bdm\"\n",
    "HDFS_HOME = \"/user/{}\".format(HDFS_USER)\n",
    "\n",
    "# For HDFS Path\n",
    "\n",
    "hdfs_home = \"{}{}\".format(HDFS_DEFAULT, HDFS_HOME)\n",
    "\n",
    "# For events\n",
    "activities_dir = join(\"data\", \"events\", \"activities\")\n",
    "culture_dir = join(\"data\", \"events\", \"culture\")\n",
    "tourist_points_dir = join(\"data\", \"events\", \"tourist_points\")\n",
    "\n",
    "# For specific file\n",
    "data_date = \"20220404\"\n",
    "\n",
    "activities_file = \"{}/{}/{}\".format(hdfs_home, activities_dir, \"activities_{}.parquet\".format(data_date))\n",
    "culture_file = \"{}/{}/{}\".format(hdfs_home, culture_dir, \"culture_{}.parquet\".format(data_date))\n",
    "tourist_points_file = \"{}/{}/{}\".format(hdfs_home, tourist_points_dir, \"tourist_points_{}.parquet\".format(data_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d9f17a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://alakazam.fib.upc.es:27000/user/bdm/data/events/activities/activities_20220404.parquet'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'hdfs://alakazam.fib.upc.es:27000/user/bdm/data/events/culture/culture_20220404.parquet'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'hdfs://alakazam.fib.upc.es:27000/user/bdm/data/events/tourist_points/tourist_points_20220404.parquet'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities_file\n",
    "culture_file\n",
    "tourist_points_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bee5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"bdm\").master('local').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a41697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activities = spark.read.format(\"parquet\").load(activities_file)\n",
    "df_culture = spark.read.format(\"parquet\").load(culture_file)\n",
    "df_tourist_points = spark.read.format(\"parquet\").load(tourist_points_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9327226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_cols = [\n",
    "    'register_id', 'name', 'geo_epgs_4326_x', 'geo_epgs_4326_y', # Must\n",
    "    'addresses_neighborhood_id', 'addresses_neighborhood_name', # For neighborhood's query\n",
    "    'addresses_district_id', 'addresses_district_name', # For district query\n",
    "    'addresses_road_name', 'addresses_road_id' # Maybe useful to search events on that road\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568b9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c10da05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[addresses_roadtype_name: int, addresses_end_street_number: bigint, values_attribute_name: string, addresses_road_name: string, values_category: string, addresses_zip_code: bigint, secondary_filters_id: bigint, values_value: string, addresses_town: string, geo_epgs_4326_y: double, geo_epgs_4326_x: double, secondary_filters_name: string, secondary_filters_tree: bigint, addresses_district_name: string, geo_epgs_25831_x: double, addresses_start_street_number: bigint, register_id: string, institution_id: bigint, addresses_main_address: boolean, addresses_district_id: bigint, addresses_roadtype_id: int, addresses_type: int, addresses_neighborhood_id: bigint, _id: bigint, name: string, addresses_road_id: bigint, created: timestamp, geo_epgs_25831_y: double, institution_name: string, modified: timestamp, secondary_filters_asia_id: bigint, secondary_filters_fullpath: string, values_description: string, values_id: bigint, addresses_neighborhood_name: string, values_outstanding: boolean, values_attribute_id: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[addresses_roadtype_name: timestamp, addresses_end_street_number: string, values_attribute_name: string, addresses_road_name: string, values_category: string, addresses_zip_code: string, secondary_filters_id: string, values_value: string, addresses_town: string, geo_epgs_4326_y: double, geo_epgs_4326_x: double, secondary_filters_name: int, secondary_filters_tree: int, addresses_district_name: string, geo_epgs_25831_x: double, addresses_start_street_number: string, register_id: string, institution_id: string, addresses_main_address: string, addresses_district_id: string, addresses_roadtype_id: timestamp, addresses_type: string, addresses_neighborhood_id: string, _id: bigint, name: string, addresses_road_id: string, created: string, geo_epgs_25831_y: double, institution_name: string, modified: timestamp, secondary_filters_asia_id: int, secondary_filters_fullpath: int, values_description: string, values_id: string, addresses_neighborhood_name: string, values_outstanding: string, values_attribute_id: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[addresses_roadtype_name: int, addresses_end_street_number: bigint, values_attribute_name: string, addresses_road_name: string, values_category: string, addresses_zip_code: bigint, secondary_filters_id: bigint, values_value: string, addresses_town: string, geo_epgs_4326_y: double, geo_epgs_4326_x: double, secondary_filters_name: string, secondary_filters_tree: bigint, addresses_district_name: string, geo_epgs_25831_x: double, addresses_start_street_number: bigint, register_id: string, institution_id: bigint, addresses_main_address: boolean, addresses_district_id: bigint, addresses_roadtype_id: int, addresses_type: int, addresses_neighborhood_id: bigint, _id: bigint, name: string, addresses_road_id: bigint, created: timestamp, geo_epgs_25831_y: double, institution_name: string, modified: timestamp, secondary_filters_asia_id: bigint, secondary_filters_fullpath: string, values_description: string, values_id: bigint, addresses_neighborhood_name: string, values_outstanding: boolean, values_attribute_id: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_activities\n",
    "df_culture\n",
    "df_tourist_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a72db9",
   "metadata": {},
   "source": [
    "#### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5daaefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dataFrame(sqlCtx, rdd, columns=None, sep=\",\", parseDate=True):\n",
    "    \"\"\"\n",
    "    Converts CSV plain text RDD into SparkSQL DataFrame (former SchemaRDD) using PySpark. If columns not given, assumes first row is the header. If separator not given, assumes comma separated\n",
    "    \"\"\"\n",
    "    py_version = sys.version_info[0]\n",
    "    if py_version < 3:\n",
    "        def toRow(line): return toRowSep(line.encode('utf-8'), sep)\n",
    "    else:\n",
    "        def toRow(line): return toRowSep(line, sep)\n",
    "\n",
    "    rdd_array = rdd.map(toRow)\n",
    "    rdd_sql = rdd_array\n",
    "\n",
    "    if columns is None:\n",
    "        columns = rdd_array.first()\n",
    "        rdd_sql = rdd_array.zipWithIndex().filter(\n",
    "            lambda r_i: r_i[1] > 0).keys()\n",
    "    column_types = evaluateType(rdd_sql, parseDate)\n",
    "\n",
    "    def toSqlRow(row):\n",
    "        return toSqlRowWithType(row, column_types)\n",
    "\n",
    "    schema = makeSchema(zip(columns, column_types))\n",
    "\n",
    "    return sqlCtx.createDataFrame(rdd_sql.map(toSqlRow), schema=schema)\n",
    "\n",
    "\n",
    "def makeSchema(columns):\n",
    "    struct_field_map = {'string': StringType(),\n",
    "                        'date': TimestampType(),\n",
    "                        'double': DoubleType(),\n",
    "                        'int': IntegerType(),\n",
    "                        'none': NullType()}\n",
    "    fields = [StructField(k, struct_field_map[v], True) for k, v in columns]\n",
    "\n",
    "    return StructType(fields)\n",
    "\n",
    "\n",
    "def toRowSep(line, d):\n",
    "    \"\"\"Parses one row using csv reader\"\"\"\n",
    "    for r in csv.reader([line], delimiter=d):\n",
    "        return r\n",
    "\n",
    "\n",
    "def toSqlRowWithType(row, col_types):\n",
    "    \"\"\"Convert to sql.Row\"\"\"\n",
    "    d = row\n",
    "    for col, data in enumerate(row):\n",
    "        typed = col_types[col]\n",
    "        if isNone(data):\n",
    "            d[col] = None\n",
    "        elif typed == 'string':\n",
    "            d[col] = data\n",
    "        elif typed == 'int':\n",
    "            d[col] = int(round(float(data)))\n",
    "        elif typed == 'double':\n",
    "            d[col] = float(data)\n",
    "        elif typed == 'date':\n",
    "            d[col] = toDate(data)\n",
    "    return d\n",
    "\n",
    "\n",
    "# Type converter\n",
    "def isNone(d):\n",
    "    return (d is None or d == 'None' or\n",
    "            d == '?' or\n",
    "            d == '' or\n",
    "            d == 'NULL' or\n",
    "            d == 'null')\n",
    "\n",
    "\n",
    "def toDate(d):\n",
    "    return dateutil.parser.parse(d)\n",
    "\n",
    "\n",
    "def getRowType(row):\n",
    "    \"\"\"Infers types for each row\"\"\"\n",
    "    d = row\n",
    "    for col, data in enumerate(row):\n",
    "        try:\n",
    "            if isNone(data):\n",
    "                d[col] = 'none'\n",
    "            else:\n",
    "                num = float(data)\n",
    "                if num.is_integer():\n",
    "                    d[col] = 'int'\n",
    "                else:\n",
    "                    d[col] = 'double'\n",
    "        except:\n",
    "            try:\n",
    "                toDate(data)\n",
    "                d[col] = 'date'\n",
    "            except:\n",
    "                d[col] = 'string'\n",
    "    return d\n",
    "\n",
    "\n",
    "def getRowTypeNoDate(row):\n",
    "    \"\"\"Infers types for each row\"\"\"\n",
    "    d = row\n",
    "    for col, data in enumerate(row):\n",
    "        try:\n",
    "            if isNone(data):\n",
    "                d[col] = 'none'\n",
    "            else:\n",
    "                num = float(data)\n",
    "                if num.is_integer():\n",
    "                    d[col] = 'int'\n",
    "                else:\n",
    "                    d[col] = 'double'\n",
    "        except:\n",
    "            d[col] = 'string'\n",
    "    return d\n",
    "\n",
    "\n",
    "def reduceTypes(a, b):\n",
    "    \"\"\"Reduces column types among rows to find common denominator\"\"\"\n",
    "    type_order = {'string': 0, 'date': 1, 'double': 2, 'int': 3, 'none': 4}\n",
    "    reduce_map = {'int': {0: 'string', 1: 'string', 2: 'double'},\n",
    "                  'double': {0: 'string', 1: 'string'},\n",
    "                  'date': {0: 'string'}}\n",
    "    d = a\n",
    "    for col, a_type in enumerate(a):\n",
    "        # a_type = a[col]\n",
    "        b_type = b[col]\n",
    "        if a_type == 'none':\n",
    "            d[col] = b_type\n",
    "        elif b_type == 'none':\n",
    "            d[col] = a_type\n",
    "        else:\n",
    "            order_a = type_order[a_type]\n",
    "            order_b = type_order[b_type]\n",
    "            if order_a == order_b:\n",
    "                d[col] = a_type\n",
    "            elif order_a > order_b:\n",
    "                d[col] = reduce_map[a_type][order_b]\n",
    "            elif order_a < order_b:\n",
    "                d[col] = reduce_map[b_type][order_a]\n",
    "    return d\n",
    "\n",
    "\n",
    "def evaluateType(rdd_sql, parseDate):\n",
    "    if parseDate:\n",
    "        return rdd_sql.map(getRowType).reduce(reduceTypes)\n",
    "    else:\n",
    "        return rdd_sql.map(getRowTypeNoDate).reduce(reduceTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5116bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(activities_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6329e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[addresses_roadtype_name: int, addresses_end_street_number: bigint, values_attribute_name: string, addresses_road_name: string, values_category: string, addresses_zip_code: bigint, secondary_filters_id: bigint, values_value: string, addresses_town: string, geo_epgs_4326_y: double, geo_epgs_4326_x: double, secondary_filters_name: string, secondary_filters_tree: bigint, addresses_district_name: string, geo_epgs_25831_x: double, addresses_start_street_number: bigint, register_id: string, institution_id: bigint, addresses_main_address: boolean, addresses_district_id: bigint, addresses_roadtype_id: int, addresses_type: int, addresses_neighborhood_id: bigint, _id: bigint, name: string, addresses_road_id: bigint, created: timestamp, geo_epgs_25831_y: double, institution_name: string, modified: timestamp, secondary_filters_asia_id: bigint, secondary_filters_fullpath: string, values_description: string, values_id: bigint, addresses_neighborhood_name: string, values_outstanding: boolean, values_attribute_id: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "133b94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.format(\"csv\").load(activities_file)\n",
    "# spark.read.csv(activities_file, sep=\",\", header=0)\n",
    "df_rdd = sc.textFile(activities_file)\n",
    "df = csv_to_dataFrame(sqlContext, df_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c0d3e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "_df = df.to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c09bb715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.pandas.frame.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fea6afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = _df.drop(columns=[\"secondary_filters_asia_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f0cf656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/31 19:30:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/31 19:30:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/31 19:30:37 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 15)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1392, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1333, in verify_integer\n",
      "    raise ValueError(\n",
      "ValueError: field secondary_filters_asia_id: object of IntegerType out of range, got: 65103003001003\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/05/31 19:30:37 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 15) (alakazam.fib.upc.es executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1392, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1333, in verify_integer\n",
      "    raise ValueError(\n",
      "ValueError: field secondary_filters_asia_id: object of IntegerType out of range, got: 65103003001003\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/05/31 19:30:37 ERROR TaskSetManager: Task 0 in stage 15.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o798.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 15) (alakazam.fib.upc.es executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1333, in verify_integer\n    raise ValueError(\nValueError: field secondary_filters_asia_id: object of IntegerType out of range, got: 65103003001003\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1333, in verify_integer\n    raise ValueError(\nValueError: field secondary_filters_asia_id: object of IntegerType out of range, got: 65103003001003\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py:707\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=699'>700</a>\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=700'>701</a>\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=701'>702</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=702'>703</a>\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=703'>704</a>\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=704'>705</a>\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=705'>706</a>\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=706'>707</a>\u001b[0m printer\u001b[39m.\u001b[39;49mpretty(obj)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=707'>708</a>\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/core/formatters.py?line=708'>709</a>\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=406'>407</a>\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=407'>408</a>\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=408'>409</a>\u001b[0m                         \u001b[39mand\u001b[39;00m callable(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=409'>410</a>\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39;49m, cycle)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=411'>412</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=412'>413</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=775'>776</a>\u001b[0m \u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=776'>777</a>\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=777'>778</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39;49m(obj)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=778'>779</a>\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/IPython/lib/pretty.py?line=779'>780</a>\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/series.py:6296\u001b[0m, in \u001b[0;36mSeries.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/series.py?line=6292'>6293</a>\u001b[0m \u001b[39mif\u001b[39;00m max_display_count \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/series.py?line=6293'>6294</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_internal_pandas()\u001b[39m.\u001b[39mto_string(name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m-> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/series.py?line=6295'>6296</a>\u001b[0m pser \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_psdf\u001b[39m.\u001b[39;49m_get_or_create_repr_pandas_cache(max_display_count)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname]\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/series.py?line=6296'>6297</a>\u001b[0m pser_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(pser)\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/series.py?line=6297'>6298</a>\u001b[0m pser \u001b[39m=\u001b[39m pser\u001b[39m.\u001b[39miloc[:max_display_count]\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py:11711\u001b[0m, in \u001b[0;36mDataFrame._get_or_create_repr_pandas_cache\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11707'>11708</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_or_create_repr_pandas_cache\u001b[39m(\u001b[39mself\u001b[39m, n: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11708'>11709</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_repr_pandas_cache\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m n \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_repr_pandas_cache:\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11709'>11710</a>\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\n\u001b[0;32m> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11710'>11711</a>\u001b[0m             \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_repr_pandas_cache\u001b[39m\u001b[39m\"\u001b[39m, {n: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(n \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49m_to_internal_pandas()}\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11711'>11712</a>\u001b[0m         )\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11712'>11713</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_repr_pandas_cache[n]\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py:11706\u001b[0m, in \u001b[0;36mDataFrame._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11699'>11700</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_internal_pandas\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11700'>11701</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11701'>11702</a>\u001b[0m \u001b[39m    Return a pandas DataFrame directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11702'>11703</a>\u001b[0m \n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11703'>11704</a>\u001b[0m \u001b[39m    This method is for internal use only.\u001b[39;00m\n\u001b[1;32m  <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11704'>11705</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/frame.py?line=11705'>11706</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal\u001b[39m.\u001b[39;49mto_pandas_frame\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/utils.py:580\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/utils.py?line=575'>576</a>\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/utils.py?line=576'>577</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/utils.py?line=577'>578</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_lazy_property\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/utils.py?line=578'>579</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, attr_name):\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/utils.py?line=579'>580</a>\u001b[0m         \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, attr_name, fn(\u001b[39mself\u001b[39;49m))\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/utils.py?line=580'>581</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/internal.py:1051\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/internal.py?line=1048'>1049</a>\u001b[0m \u001b[39m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/internal.py?line=1049'>1050</a>\u001b[0m sdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/internal.py?line=1050'>1051</a>\u001b[0m pdf \u001b[39m=\u001b[39m sdf\u001b[39m.\u001b[39;49mtoPandas()\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/internal.py?line=1051'>1052</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pdf) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(sdf\u001b[39m.\u001b[39mschema) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/internal.py?line=1052'>1053</a>\u001b[0m     pdf \u001b[39m=\u001b[39m pdf\u001b[39m.\u001b[39mastype(\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/internal.py?line=1053'>1054</a>\u001b[0m         {field\u001b[39m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m sdf\u001b[39m.\u001b[39mschema}\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/pandas/internal.py?line=1054'>1055</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:157\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py?line=153'>154</a>\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py?line=155'>156</a>\u001b[0m \u001b[39m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py?line=156'>157</a>\u001b[0m pdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect(), columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py?line=157'>158</a>\u001b[0m column_counter \u001b[39m=\u001b[39m Counter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py?line=159'>160</a>\u001b[0m dtype \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=682'>683</a>\u001b[0m \u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=683'>684</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=684'>685</a>\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=689'>690</a>\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=690'>691</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=691'>692</a>\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc) \u001b[39mas\u001b[39;00m css:\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=692'>693</a>\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=693'>694</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o798.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 15) (alakazam.fib.upc.es executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1333, in verify_integer\n    raise ValueError(\nValueError: field secondary_filters_asia_id: object of IntegerType out of range, got: 65103003001003\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/types.py\", line 1333, in verify_integer\n    raise ValueError(\nValueError: field secondary_filters_asia_id: object of IntegerType out of range, got: 65103003001003\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "_df['addresses_roadtype_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33c4a406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "      <th>...</th>\n",
       "      <th>_c27</th>\n",
       "      <th>_c28</th>\n",
       "      <th>_c29</th>\n",
       "      <th>_c30</th>\n",
       "      <th>_c31</th>\n",
       "      <th>_c32</th>\n",
       "      <th>_c33</th>\n",
       "      <th>_c34</th>\n",
       "      <th>_c35</th>\n",
       "      <th>_c36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>addresses_roadtype_name</td>\n",
       "      <td>addresses_end_street_number</td>\n",
       "      <td>values_attribute_name</td>\n",
       "      <td>addresses_road_name</td>\n",
       "      <td>values_category</td>\n",
       "      <td>addresses_zip_code</td>\n",
       "      <td>secondary_filters_id</td>\n",
       "      <td>values_value</td>\n",
       "      <td>addresses_town</td>\n",
       "      <td>geo_epgs_4326_y</td>\n",
       "      <td>...</td>\n",
       "      <td>geo_epgs_25831_y</td>\n",
       "      <td>institution_name</td>\n",
       "      <td>modified</td>\n",
       "      <td>secondary_filters_asia_id</td>\n",
       "      <td>secondary_filters_fullpath</td>\n",
       "      <td>values_description</td>\n",
       "      <td>values_id</td>\n",
       "      <td>addresses_neighborhood_name</td>\n",
       "      <td>values_outstanding</td>\n",
       "      <td>values_attribute_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tel.</td>\n",
       "      <td>Av Estadi</td>\n",
       "      <td>Telèfons</td>\n",
       "      <td>8038</td>\n",
       "      <td>56732071</td>\n",
       "      <td>934255445</td>\n",
       "      <td>BARCELONA</td>\n",
       "      <td>2.146913285556539</td>\n",
       "      <td>...</td>\n",
       "      <td>4579669.4380555395</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-03-15T18:19:11.887664</td>\n",
       "      <td>65103003001003</td>\n",
       "      <td>Planol BCN &gt;&gt; Educació &gt;&gt; Ensenyament reglat &gt;...</td>\n",
       "      <td>None</td>\n",
       "      <td>166525</td>\n",
       "      <td>el Poble-sec</td>\n",
       "      <td>True</td>\n",
       "      <td>20001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _c0                          _c1  \\\n",
       "0  addresses_roadtype_name  addresses_end_street_number   \n",
       "1                     None                         None   \n",
       "\n",
       "                     _c2                  _c3              _c4  \\\n",
       "0  values_attribute_name  addresses_road_name  values_category   \n",
       "1                   Tel.            Av Estadi         Telèfons   \n",
       "\n",
       "                  _c5                   _c6           _c7             _c8  \\\n",
       "0  addresses_zip_code  secondary_filters_id  values_value  addresses_town   \n",
       "1                8038              56732071     934255445       BARCELONA   \n",
       "\n",
       "                 _c9  ...                _c27              _c28  \\\n",
       "0    geo_epgs_4326_y  ...    geo_epgs_25831_y  institution_name   \n",
       "1  2.146913285556539  ...  4579669.4380555395              None   \n",
       "\n",
       "                         _c29                       _c30  \\\n",
       "0                    modified  secondary_filters_asia_id   \n",
       "1  2022-03-15T18:19:11.887664             65103003001003   \n",
       "\n",
       "                                                _c31                _c32  \\\n",
       "0                         secondary_filters_fullpath  values_description   \n",
       "1  Planol BCN >> Educació >> Ensenyament reglat >...                None   \n",
       "\n",
       "        _c33                         _c34                _c35  \\\n",
       "0  values_id  addresses_neighborhood_name  values_outstanding   \n",
       "1     166525                 el Poble-sec                True   \n",
       "\n",
       "                  _c36  \n",
       "0  values_attribute_id  \n",
       "1                20001  \n",
       "\n",
       "[2 rows x 37 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7df4ae92",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkConf' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bdm/BDM/src/notebooks/test.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e31227d/home/bdm/BDM/src/notebooks/test.ipynb#ch0000009vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# spark = SparkSession.builder.master(\"local\").appName(\"Pi\").config(sc.getConf()).getOrCreate()\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e31227d/home/bdm/BDM/src/notebooks/test.ipynb#ch0000009vscode-remote?line=1'>2</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py:226\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py?line=223'>224</a>\u001b[0m sparkConf \u001b[39m=\u001b[39m SparkConf()\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py?line=224'>225</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py?line=225'>226</a>\u001b[0m     sparkConf\u001b[39m.\u001b[39;49mset(key, value)\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py?line=226'>227</a>\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/session.py?line=227'>228</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(sparkConf)\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/conf.py:131\u001b[0m, in \u001b[0;36mSparkConf.set\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/conf.py?line=128'>129</a>\u001b[0m \u001b[39m# Try to set self._jconf first if JVM is created, set self._conf if JVM is not created yet.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/conf.py?line=129'>130</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/conf.py?line=130'>131</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jconf\u001b[39m.\u001b[39;49mset(key, \u001b[39mstr\u001b[39;49m(value))\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/conf.py?line=131'>132</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/conf.py?line=132'>133</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf[key] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(value)\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py:1313\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1311'>1312</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m-> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1312'>1313</a>\u001b[0m     args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_args(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m     command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m         args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m         proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py:1283\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1278'>1279</a>\u001b[0m     new_args \u001b[39m=\u001b[39m args\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1279'>1280</a>\u001b[0m     temp_args \u001b[39m=\u001b[39m []\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1281'>1282</a>\u001b[0m args_command \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m-> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1282'>1283</a>\u001b[0m     [get_command_part(arg, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m new_args])\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1284'>1285</a>\u001b[0m \u001b[39mreturn\u001b[39;00m args_command, temp_args\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py:1283\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1278'>1279</a>\u001b[0m     new_args \u001b[39m=\u001b[39m args\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1279'>1280</a>\u001b[0m     temp_args \u001b[39m=\u001b[39m []\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1281'>1282</a>\u001b[0m args_command \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m-> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1282'>1283</a>\u001b[0m     [get_command_part(arg, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m new_args])\n\u001b[1;32m   <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py?line=1284'>1285</a>\u001b[0m \u001b[39mreturn\u001b[39;00m args_command, temp_args\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py:298\u001b[0m, in \u001b[0;36mget_command_part\u001b[0;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=295'>296</a>\u001b[0m         command_part \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m interface\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=296'>297</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=297'>298</a>\u001b[0m     command_part \u001b[39m=\u001b[39m REFERENCE_TYPE \u001b[39m+\u001b[39m parameter\u001b[39m.\u001b[39;49m_get_object_id()\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=299'>300</a>\u001b[0m command_part \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py?line=301'>302</a>\u001b[0m \u001b[39mreturn\u001b[39;00m command_part\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkConf' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.master(\"local\").appName(\"Pi\").config(sc.getConf()).getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9488140e-4247-45d6-8532-84e5c03a7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5785fa8-57c6-4983-9bbe-283085d4aac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe16ac06b6d4bacd07cf5564ffa86eb58e7e2ab169d83d01490a955ddfe1246"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bdm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
