{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1775511a-83f0-4d6e-9611-2e27291144ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # disable warnings\n",
    "\n",
    "from os import getcwd\n",
    "from os.path import join, abspath, pardir, exists\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import (StringType, DoubleType, TimestampType, NullType, IntegerType, StructType, StructField)\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3cf65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For IPython\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # To show all output after each cell execution (instead of the last output)\n",
    "\n",
    "# For pandas\n",
    "\n",
    "pd.options.display.max_columns = 200 # display upto 200 columns (instead of default 20)\n",
    "pd.options.display.max_rows = 200 # display upto 200 rows (instead of default 60)\n",
    "pd.options.mode.use_inf_as_na = True # Consider '' and np.inf as np.NaN values (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e42258",
   "metadata": {},
   "source": [
    "Check the official doc for FPGrowth [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.fpm.FPGrowth.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c24df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/07 12:56:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"bdm\").master('local').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81edf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|       items|\n",
      "+---+------------+\n",
      "|  0|   [1, 2, 5]|\n",
      "|  1|[1, 2, 3, 5]|\n",
      "|  2|      [1, 2]|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, [1, 2, 5]),\n",
    "    (1, [1, 2, 3, 5]),\n",
    "    (2, [1, 2])\n",
    "], [\"id\", \"items\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb97904b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itemsCol: items column name (default: items, current: items)\\nminConfidence: Minimal confidence for generating Association Rule. [0.0, 1.0]. minConfidence will not affect the mining for frequent itemsets, but will affect the association rules generation. (default: 0.8, current: 0.6)\\nminSupport: Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears more than (minSupport * size-of-the-dataset) times will be output in the frequent itemsets. (default: 0.3, current: 0.5)\\nnumPartitions: Number of partitions (at least 1) used by parallel FP-growth. By default the param is not set, and partition number of the input dataset is used. (undefined)\\npredictionCol: prediction column name. (default: prediction)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
    "fpGrowth.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e5a8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|    items|freq|\n",
      "+---------+----+\n",
      "|      [5]|   2|\n",
      "|   [5, 2]|   2|\n",
      "|[5, 2, 1]|   2|\n",
      "|   [5, 1]|   2|\n",
      "|      [2]|   3|\n",
      "|   [2, 1]|   3|\n",
      "|      [1]|   3|\n",
      "+---------+----+\n",
      "\n",
      "+----------+----------+------------------+----+------------------+\n",
      "|antecedent|consequent|        confidence|lift|           support|\n",
      "+----------+----------+------------------+----+------------------+\n",
      "|    [5, 2]|       [1]|               1.0| 1.0|0.6666666666666666|\n",
      "|    [5, 1]|       [2]|               1.0| 1.0|0.6666666666666666|\n",
      "|       [5]|       [2]|               1.0| 1.0|0.6666666666666666|\n",
      "|       [5]|       [1]|               1.0| 1.0|0.6666666666666666|\n",
      "|       [2]|       [5]|0.6666666666666666| 1.0|0.6666666666666666|\n",
      "|       [2]|       [1]|               1.0| 1.0|               1.0|\n",
      "|       [1]|       [5]|0.6666666666666666| 1.0|0.6666666666666666|\n",
      "|       [1]|       [2]|               1.0| 1.0|               1.0|\n",
      "|    [2, 1]|       [5]|0.6666666666666666| 1.0|0.6666666666666666|\n",
      "+----------+----------+------------------+----+------------------+\n",
      "\n",
      "+---+------------+----------+\n",
      "| id|       items|prediction|\n",
      "+---+------------+----------+\n",
      "|  0|   [1, 2, 5]|        []|\n",
      "|  1|[1, 2, 3, 5]|        []|\n",
      "|  2|      [1, 2]|       [5]|\n",
      "+---+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = fpGrowth.fit(df)\n",
    "\n",
    "# Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "model.associationRules.show()\n",
    "\n",
    "# transform examines the input items against all the association rules and summarize the\n",
    "# consequents as prediction\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb161dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe16ac06b6d4bacd07cf5564ffa86eb58e7e2ab169d83d01490a955ddfe1246"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bdm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
