{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40395952-efee-4ce9-a21b-fe326add3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # disable warnings\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import csv, sys\n",
    "import dateutil.parser\n",
    "import pyspark as ps\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import (StringType, DoubleType, TimestampType, NullType, IntegerType, StructType, StructField)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed05d9",
   "metadata": {},
   "source": [
    "Config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a97ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For IPython\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # To show all output after each cell execution (instead of the last output)\n",
    "\n",
    "# For HDFS\n",
    "HDFS_DEFAULT = \"hdfs://alakazam.fib.upc.es:27000\"\n",
    "HDFS_USER = \"bdm\"\n",
    "HDFS_HOME = \"/user/{}\".format(HDFS_USER)\n",
    "\n",
    "# For HDFS Path\n",
    "\n",
    "hdfs_home = \"{}{}\".format(HDFS_DEFAULT, HDFS_HOME)\n",
    "\n",
    "# For events\n",
    "activities_dir = join(\"data\", \"events\", \"activities\")\n",
    "culture_dir = join(\"data\", \"events\", \"culture\")\n",
    "tourist_points_dir = join(\"data\", \"events\", \"tourist_points\")\n",
    "\n",
    "# For specific file\n",
    "data_date = \"20220404\"\n",
    "\n",
    "activities_file = \"{}/{}/{}\".format(hdfs_home, activities_dir, \"activities_{}.parquet\".format(data_date))\n",
    "culture_file = \"{}/{}/{}\".format(hdfs_home, culture_dir, \"culture_{}.parquet\".format(data_date))\n",
    "tourist_points_file = \"{}/{}/{}\".format(hdfs_home, tourist_points_dir, \"tourist_points_{}.parquet\".format(data_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bee5a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/22 00:29:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"bdm5\").master('local').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e6c471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['satisfaction_level,last_evaluation,number_project,average_montly_hours,time_spend_company,Work_accident,left,promotion_last_5years,sales,salary',\n",
       " '0.38,0.53,2,157,3,0,1,0,sales,low',\n",
       " '0.8,0.86,5,262,6,0,1,0,sales,medium',\n",
       " '0.11,0.88,7,272,4,0,1,0,sales,medium',\n",
       " '0.72,0.87,5,223,5,0,1,0,sales,low',\n",
       " '0.37,0.52,2,159,3,0,1,0,sales,low',\n",
       " '0.41,0.5,2,153,3,0,1,0,sales,low',\n",
       " '0.1,0.77,6,247,4,0,1,0,sales,low',\n",
       " '0.92,0.85,5,259,5,0,1,0,sales,low',\n",
       " '0.89,1,5,224,5,0,1,0,sales,low']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = sc.textFile(\"hr_comma_sep.csv\")\n",
    "file.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e36a1e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_computeFractionForSampleSize',\n",
       " '_defaultReducePartitions',\n",
       " '_id',\n",
       " '_is_barrier',\n",
       " '_jrdd',\n",
       " '_jrdd_deserializer',\n",
       " '_memory_limit',\n",
       " '_pickled',\n",
       " '_reserialize',\n",
       " '_to_java_object_rdd',\n",
       " 'aggregate',\n",
       " 'aggregateByKey',\n",
       " 'barrier',\n",
       " 'cache',\n",
       " 'cartesian',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'cogroup',\n",
       " 'collect',\n",
       " 'collectAsMap',\n",
       " 'collectWithJobGroup',\n",
       " 'combineByKey',\n",
       " 'context',\n",
       " 'count',\n",
       " 'countApprox',\n",
       " 'countApproxDistinct',\n",
       " 'countByKey',\n",
       " 'countByValue',\n",
       " 'ctx',\n",
       " 'distinct',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatMap',\n",
       " 'flatMapValues',\n",
       " 'fold',\n",
       " 'foldByKey',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'fullOuterJoin',\n",
       " 'getCheckpointFile',\n",
       " 'getNumPartitions',\n",
       " 'getResourceProfile',\n",
       " 'getStorageLevel',\n",
       " 'glom',\n",
       " 'groupBy',\n",
       " 'groupByKey',\n",
       " 'groupWith',\n",
       " 'has_resource_profile',\n",
       " 'histogram',\n",
       " 'id',\n",
       " 'intersection',\n",
       " 'isCheckpointed',\n",
       " 'isEmpty',\n",
       " 'isLocallyCheckpointed',\n",
       " 'is_cached',\n",
       " 'is_checkpointed',\n",
       " 'join',\n",
       " 'keyBy',\n",
       " 'keys',\n",
       " 'leftOuterJoin',\n",
       " 'localCheckpoint',\n",
       " 'lookup',\n",
       " 'map',\n",
       " 'mapPartitions',\n",
       " 'mapPartitionsWithIndex',\n",
       " 'mapPartitionsWithSplit',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'meanApprox',\n",
       " 'min',\n",
       " 'name',\n",
       " 'partitionBy',\n",
       " 'partitioner',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'randomSplit',\n",
       " 'reduce',\n",
       " 'reduceByKey',\n",
       " 'reduceByKeyLocally',\n",
       " 'repartition',\n",
       " 'repartitionAndSortWithinPartitions',\n",
       " 'rightOuterJoin',\n",
       " 'sample',\n",
       " 'sampleByKey',\n",
       " 'sampleStdev',\n",
       " 'sampleVariance',\n",
       " 'saveAsHadoopDataset',\n",
       " 'saveAsHadoopFile',\n",
       " 'saveAsNewAPIHadoopDataset',\n",
       " 'saveAsNewAPIHadoopFile',\n",
       " 'saveAsPickleFile',\n",
       " 'saveAsSequenceFile',\n",
       " 'saveAsTextFile',\n",
       " 'setName',\n",
       " 'sortBy',\n",
       " 'sortByKey',\n",
       " 'stats',\n",
       " 'stdev',\n",
       " 'subtract',\n",
       " 'subtractByKey',\n",
       " 'sum',\n",
       " 'sumApprox',\n",
       " 'take',\n",
       " 'takeOrdered',\n",
       " 'takeSample',\n",
       " 'toDF',\n",
       " 'toDebugString',\n",
       " 'toLocalIterator',\n",
       " 'top',\n",
       " 'treeAggregate',\n",
       " 'treeReduce',\n",
       " 'union',\n",
       " 'unpersist',\n",
       " 'values',\n",
       " 'variance',\n",
       " 'withResources',\n",
       " 'zip',\n",
       " 'zipWithIndex',\n",
       " 'zipWithUniqueId']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5785fa8-57c6-4983-9bbe-283085d4aac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'mapToPair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bdm/BDM/src/notebooks/test_spark_rdd.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e31227d/home/bdm/BDM/src/notebooks/test_spark_rdd.ipynb#ch0000036vscode-remote?line=0'>1</a>\u001b[0m file\u001b[39m.\u001b[39;49mfilter(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m\"\u001b[39;49m\u001b[39msatisfaction_level\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m x) \\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e31227d/home/bdm/BDM/src/notebooks/test_spark_rdd.ipynb#ch0000036vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m.\u001b[39;49mmapToPair(\u001b[39mlambda\u001b[39;00m x: (\u001b[39mint\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m2\u001b[39m]), \u001b[39mfloat\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]))) \\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e31227d/home/bdm/BDM/src/notebooks/test_spark_rdd.ipynb#ch0000036vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m.\u001b[39mtake(\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'mapToPair'"
     ]
    }
   ],
   "source": [
    "file.filter(lambda x: \"satisfaction_level\" not in x) \\\n",
    ".mapToPair(lambda x: (int(x.split(\",\")[2]), float(x.split(\",\")[0]))) \\\n",
    ".take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792324c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe16ac06b6d4bacd07cf5564ffa86eb58e7e2ab169d83d01490a955ddfe1246"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bdm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
