{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40395952-efee-4ce9-a21b-fe326add3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # disable warnings\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import csv, sys\n",
    "import dateutil.parser\n",
    "import pyspark as ps\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import (StringType, DoubleType, TimestampType, NullType, IntegerType, StructType, StructField)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed05d9",
   "metadata": {},
   "source": [
    "Config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a97ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For IPython\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # To show all output after each cell execution (instead of the last output)\n",
    "\n",
    "# For HDFS\n",
    "HDFS_DEFAULT = \"hdfs://alakazam.fib.upc.es:27000\"\n",
    "HDFS_USER = \"bdm\"\n",
    "HDFS_HOME = \"/user/{}\".format(HDFS_USER)\n",
    "\n",
    "# For HDFS Path\n",
    "\n",
    "hdfs_home = \"{}{}\".format(HDFS_DEFAULT, HDFS_HOME)\n",
    "\n",
    "# For events\n",
    "activities_dir = join(\"data\", \"events\", \"activities\")\n",
    "culture_dir = join(\"data\", \"events\", \"culture\")\n",
    "tourist_points_dir = join(\"data\", \"events\", \"tourist_points\")\n",
    "\n",
    "# For specific file\n",
    "data_date = \"20220404\"\n",
    "\n",
    "activities_file = \"{}/{}/{}\".format(hdfs_home, activities_dir, \"activities_{}.parquet\".format(data_date))\n",
    "culture_file = \"{}/{}/{}\".format(hdfs_home, culture_dir, \"culture_{}.parquet\".format(data_date))\n",
    "tourist_points_file = \"{}/{}/{}\".format(hdfs_home, tourist_points_dir, \"tourist_points_{}.parquet\".format(data_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bee5a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/22 10:32:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"bdm5\").master('local').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9277acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = sc.textFile(\"log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0e93306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def red(x):\n",
    "    x[2] = \"Important {}\".format(x[2]) if x[2] in [\"ERROR\"] else x[2]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a7ab8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['20150323', '0833', 'Important ERROR', 'Oracle'],\n",
       " ['20150323', '0835', 'WARNING', 'MySQL'],\n",
       " ['20150323', '0839', 'WARNING', 'MySQL']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.map(lambda x: x.split(\";\"))\\\n",
    ".filter(lambda x: x[2] in [\"ERROR\", \"WARNING\"])\\\n",
    ".map(lambda x: red(x))\\\n",
    ".take(3)\n",
    "#.map(lambda x: (x, \"Important\" if x[2] in [\"ERROR\"] else \"\"))\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06620143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMP1,CARME,400000,MATARO,DEPT1,PROJ1',\n",
       " 'EMP2,EULALIA,150000,BARCELONA,DEPT2,PROJ1',\n",
       " 'EMP3,MIQUEL,125000,BADALONA,DEPT1,PROJ3']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['DEPT1,MANAGEMENT,10,PAU CLARIS,BARCELONA ',\n",
       " 'DEPT2,MANAGEMENT,8,RIOS ROSAS,MADRID ',\n",
       " 'DEPT4,MARKETING,3,RIOS ROSAS,MADRID']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = sc.textFile(\"emp.txt\")\n",
    "dept = sc.textFile(\"dept.txt\")\n",
    "\n",
    "emp.take(3)\n",
    "dept.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5eaaaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEPT1', 'PROJ1', 'EMP1'),\n",
       " ('DEPT2', 'PROJ1', 'EMP2'),\n",
       " ('DEPT1', 'PROJ3', 'EMP3')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.map(lambda x: (x.split(\",\")[4], x.split(\",\")[5], x.split(\",\")[0]))\\\n",
    ".take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "786210c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr(x, y):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f08a84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/22 11:17:37 ERROR Executor: Exception in task 0.0 in stage 57.0 (TID 45)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "ValueError: too many values to unpack (expected 2)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/22 11:17:37 WARN TaskSetManager: Lost task 0.0 in stage 57.0 (TID 45) (alakazam.fib.upc.es executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 417, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2146, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "ValueError: too many values to unpack (expected 2)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/06/22 11:17:37 ERROR TaskSetManager: Task 0 in stage 57.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 45) (alakazam.fib.upc.es executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/bdm/BDM/src/notebooks/test_spark_rdd.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e31227d/home/bdm/BDM/src/notebooks/test_spark_rdd.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m emp\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m x: (x\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m4\u001b[39;49m], x\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m5\u001b[39;49m], x\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m]))\\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e31227d/home/bdm/BDM/src/notebooks/test_spark_rdd.ipynb#ch0000019vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m.\u001b[39;49mreduceByKey(\u001b[39mlambda\u001b[39;49;00m a, b: rr(a, b))\\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e31227d/home/bdm/BDM/src/notebooks/test_spark_rdd.ipynb#ch0000019vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m.\u001b[39;49mtake(\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py:1568\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1565\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1567\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1568\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[1;32m   1570\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[1;32m   1571\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/context.py:1227\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[39m# Implementation note: This is implemented as a mapPartitions followed\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[39m# by runJob() in order to avoid having to pass a Python lambda into\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m \u001b[39m# SparkContext#runJob.\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m-> 1227\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[1;32m   1228\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/envs/bdm/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 45) (alakazam.fib.upc.es executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2918, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 417, in func\n    return f(iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/rdd.py\", line 2146, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/bdm/miniconda3/envs/bdm/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "emp.map(lambda x: (x.split(\",\")[4], x.split(\",\")[5], x.split(\",\")[0]))\\\n",
    ".reduceByKey(lambda a, b: rr(a, b))\\\n",
    ".take(3)\n",
    "#.reduceByKey(lambda a, b: (a[0], b[0]) if a[0] == b[0] else (None,None))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f3e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3378e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = emp.map(lambda x: (x.split(\";\")[4], x.split(\";\")))\n",
    "d = dept.map(lambda x: x.split(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70d299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = e.join(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee39aef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DPT1', (['EMP1', 'CARME', '400000', 'MATARO', 'DPT1'], 'DIRECCIO')),\n",
       " ('DPT3', (['EMP3', 'JOSEP', '250000', 'SITGES', 'DPT3'], 'MARKETING')),\n",
       " ('DPT5', (['EMP5', 'EULALIA', '150000', 'BARCELONA', 'DPT5'], 'VENDES'))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.map(lambda x: (x[0], (x[1][0], x[1][1])))\\\n",
    ".take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e6c471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['satisfaction_level,last_evaluation,number_project,average_montly_hours,time_spend_company,Work_accident,left,promotion_last_5years,sales,salary',\n",
       " '0.38,0.53,2,157,3,0,1,0,sales,low',\n",
       " '0.8,0.86,5,262,6,0,1,0,sales,medium',\n",
       " '0.11,0.88,7,272,4,0,1,0,sales,medium',\n",
       " '0.72,0.87,5,223,5,0,1,0,sales,low',\n",
       " '0.37,0.52,2,159,3,0,1,0,sales,low',\n",
       " '0.41,0.5,2,153,3,0,1,0,sales,low',\n",
       " '0.1,0.77,6,247,4,0,1,0,sales,low',\n",
       " '0.92,0.85,5,259,5,0,1,0,sales,low',\n",
       " '0.89,1,5,224,5,0,1,0,sales,low']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = sc.textFile(\"hr_comma_sep.csv\")\n",
    "file.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5785fa8-57c6-4983-9bbe-283085d4aac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (0.38, 1)),\n",
       " (5, (0.8, 1)),\n",
       " (7, (0.11, 1)),\n",
       " (5, (0.72, 1)),\n",
       " (2, (0.37, 1)),\n",
       " (2, (0.41, 1)),\n",
       " (6, (0.1, 1)),\n",
       " (5, (0.92, 1)),\n",
       " (5, (0.89, 1)),\n",
       " (2, (0.42, 1))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = file.filter(lambda x: \"satisfaction_level\" not in x) \\\n",
    ".map(lambda x: (int(x.split(\",\")[2]), float(x.split(\",\")[0]))) \\\n",
    ".mapValues(lambda x: (x, 1))\n",
    "\n",
    "r.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3792324c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (1143.300000000009, 2388)),\n",
       " (5, (1874.4100000000037, 2761)),\n",
       " (7, (30.39000000000001, 256)),\n",
       " (6, (321.04000000000235, 1174)),\n",
       " (4, (3034.2499999999973, 4365)),\n",
       " (3, (2788.5000000000027, 4055))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = r.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "d.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33c1916c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6951317296678116, 4),\n",
       " (0.6876695437731203, 3),\n",
       " (0.6788880840275276, 5),\n",
       " (0.4787688442211093, 2),\n",
       " (0.27345826235093895, 6),\n",
       " (0.11871093750000004, 7)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.mapValues(lambda x: x[0] / x[1]) \\\n",
    ".map(lambda x: (x[1], x[0])) \\\n",
    ".sortByKey(ascending=False) \\\n",
    ".take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fbbe8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6951317296678116, 4),\n",
       " (0.6876695437731203, 3),\n",
       " (0.6788880840275276, 5),\n",
       " (0.4787688442211093, 2),\n",
       " (0.27345826235093895, 6),\n",
       " (0.11871093750000004, 7)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.map(lambda x: ( x[1][0]/x[1][1] ,x[0] ))\\\n",
    ".sortByKey(ascending=False) \\\n",
    ".take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52c316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe16ac06b6d4bacd07cf5564ffa86eb58e7e2ab169d83d01490a955ddfe1246"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bdm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
